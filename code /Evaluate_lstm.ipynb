{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98e488c-0dde-4e00-aa1c-e0c540b7b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pack_sequence,pad_packed_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc174d66-d52f-446f-a5a2-960729246cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, trainX, trainY):\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return torch.FloatTensor(self.trainX[idx]), torch.FloatTensor(self.trainY[idx])\n",
    "        return self.trainX[idx], self.trainY[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        assert np.array(self.trainX).shape[0] == np.array(self.trainY).shape[0]\n",
    "        return np.array(self.trainX).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9a6a4d-7e30-4824-892e-9c26c49b7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(output, target):\n",
    "    target_hat = torch.max(output, dim=1).indices\n",
    "    correct_num = int(sum(target_hat==target))\n",
    "    total_num = target.shape[0]\n",
    "    return correct_num, total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eccdc9d-88c9-4250-af09-cb76b1e528af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    # accuracy\n",
    "    with torch.no_grad():\n",
    "        correct_list = []\n",
    "        total_list = []\n",
    "        for x, y in tqdm(data_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs = lstm(x, device)\n",
    "            correct_num, total_num = compute_accuracy(outputs, y)\n",
    "            correct_list.append(correct_num)\n",
    "            total_list.append(total_num)\n",
    "        \n",
    "        accuracy = sum(correct_list) / sum(total_list)\n",
    "        \n",
    " \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2c8d60-d887-4f9c-b218-c551fe972a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_F1(model, data_loader):\n",
    "    \n",
    "    labelencoder = joblib.load('labelencoder.pkl')\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        matrix_dict = {}\n",
    "        \n",
    "        for x, y in tqdm(data_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = lstm(x, device)\n",
    "    \n",
    "            target_hat = str(int(torch.max(outputs, dim=1).indices))\n",
    "            target = str(int(y))\n",
    "            if target_hat not in matrix_dict:\n",
    "                matrix_dict[target_hat] = {'TP':0, 'FP':0, 'FN':0}\n",
    "            if target not in matrix_dict:\n",
    "                matrix_dict[target] = {'TP':0, 'FP':0, 'FN':0}\n",
    "                \n",
    "            if target_hat == target:\n",
    "                matrix_dict[target_hat]['TP'] += 1\n",
    "            else:\n",
    "                matrix_dict[target_hat]['FP'] += 1\n",
    "                matrix_dict[target]['FN'] += 1\n",
    "                \n",
    "        f1_dict = {}\n",
    "        for encoded_label in matrix_dict:\n",
    "            true_label = labelencoder.inverse_transform([int(encoded_label)])[0]\n",
    "            TP = matrix_dict[encoded_label]['TP']\n",
    "            FP = matrix_dict[encoded_label]['FP']\n",
    "            FN = matrix_dict[encoded_label]['FN']\n",
    "            \n",
    "            if TP == 0:\n",
    "                precision =0\n",
    "                recall = 0\n",
    "                F1_score = 0\n",
    "            else:\n",
    "                precision = TP/(TP+FP)\n",
    "                recall = TP/(TP+FN)\n",
    "                F1_score = 2 * (precision * recall)/(precision + recall)            \n",
    "    \n",
    "            f1_dict[true_label] = {'precision':precision, 'recall':recall, 'f1-score':F1_score}\n",
    "            \n",
    "    # Macro-F1 : an arithmetic mean of the per-class F1-scores\n",
    "    sum_F1 = 0\n",
    "    for label in f1_dict:\n",
    "        sum_F1 += f1_dict[label]['f1-score']\n",
    "    macro_f1 = sum_F1 / len(f1_dict)\n",
    "    \n",
    "    return macro_f1, f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d136c17-d892-4c58-89c8-ebd517004bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, batch_size, bi):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.bi = 2 if bi == True else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=bi)\n",
    "        \n",
    "        self.fc = nn.Linear(self.bi*hidden_size, num_classes)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers * self.bi, self.batch_size, self.hidden_size))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers* self.bi, self.batch_size, self.hidden_size))\n",
    "        \n",
    "        h_0 = h_0.to(device)\n",
    "        c_0 = c_0.to(device)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # h_out = h_out.view(-1, self.hidden_size)\n",
    "        h_out = ula[:, -1, :]\n",
    "        # h_out = h_out.view(-1, self.hidden_size)\n",
    "\n",
    "\n",
    "        \n",
    "        out = self.fc(h_out)\n",
    "        # out = self.softmax(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20b4719-2e01-4167-907a-c0ace7737835",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'augmented_mfcc24_npy_files/'\n",
    "with open(base_path+'testX.npy', 'rb') as f:\n",
    "    testX = np.load(f, allow_pickle=True)\n",
    "with open(base_path+'testY.npy', 'rb') as f:\n",
    "    testY = np.load(f, allow_pickle=True)\n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "train_post_competition = pd.read_csv('augmented_train_post_competition.csv')\n",
    "data_loader_test = DataLoader(MyDataset(testX, testY), batch_size=1, shuffle=True, drop_last=True)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f847bd-e55d-4eb4-a439-1902d8bfb8fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'CNN' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-561545dc76eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ckp/cnn_3conv_3linear_20epoch_0.01lr.ckp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmacro_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_F1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'CNN' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "lstm = torch.load('ckp/cnn_3conv_3linear_20epoch_0.01lr.ckp')\n",
    "\n",
    "print('Testing')\n",
    "test_accuracy = test(lstm, data_loader_test)\n",
    "macro_f1, f1_dict = compute_F1(lstm, data_loader_test)\n",
    "print('Test accuracy: {0:.2f}'.format(test_accuracy))\n",
    "print('Test F1-score : {0:.2f}'.format(macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbf8e5-1c73-4c32-9c0a-159257add1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall-f1 matric\n",
    "\n",
    "pd.merge(pd.DataFrame.from_dict(f1_dict).T, pd.DataFrame(train_post_competition.label.value_counts()), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc9dee-96a8-4aed-931d-1b794e58a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # define a 2D convolutional layer\n",
    "            Conv2d(1, 32, kernel_size=(5, 5), stride=1, padding=(0,0)),\n",
    "            ReLU(inplace=True),\n",
    "            BatchNorm2d(32),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(32, 16, kernel_size=(3, 3), stride=1),\n",
    "            ReLU(inplace=True),\n",
    "            # BatchNorm2d(16),\n",
    "            # MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(16, 8, kernel_size=(3, 3), stride=1),\n",
    "            ReLU(inplace=True),\n",
    "            BatchNorm2d(8),\n",
    "        )\n",
    "                \n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            \n",
    "            \n",
    "            # Linear(3839840, 10000),\n",
    "            # Linear(10000, 1000),\n",
    "            \n",
    "            Linear(1000,256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(inplace=True),\n",
    "            # Dropout(0.2),\n",
    "            \n",
    "            Linear(256, 128),\n",
    "            BatchNorm1d(128),\n",
    "            ReLU(inplace=True),\n",
    "            # Dropout(0.1),\n",
    "            \n",
    "            Linear(128, 41),\n",
    "            Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # Defining the forward pass\n",
    "    def forward(self, x):\n",
    "        # x = self.cnn_layers(x)\n",
    "        x = self.vgg16(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        # print(x.shape)\n",
    "        # print(torch.max(x, dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a8ce1-44bd-4db3-b3e7-ba824d986d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
